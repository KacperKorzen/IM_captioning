{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a69ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import utiles \n",
    "import utiles_va\n",
    "from os import listdir\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Embedding, LSTM, add, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed10de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_COCO_TOKENS = '/home2/data/images/coco2017/annotations'\n",
    "PATH_FILCKR30k_TOKENS = '/home2/Kacper_captioning/flicker_30k/annotations/captions.txt'\n",
    "PATH_COCO_IMAGES = '/home2/data/images/coco2017'\n",
    "PATH_FILCKR30k_IMAGES = '/home2/data/images/flickr30k/Images/flickr30k_images'\n",
    "\n",
    "PATH_FLICKR8k_IMAGES = '/home2/data/images/flickr8k/Images'\n",
    "PATH_FILCKR8k_TOKENS = '/home2/data/images/flickr8k/captions.txt'\n",
    "\n",
    "NUM_WORDS = 10_000\n",
    "MARK_START = 'ssss'\n",
    "MARK_END = 'eeee'\n",
    "SEED = 3\n",
    "BATCH_SIZE = 512#128 #256\n",
    "EMBEDDING_SIZE = 256\n",
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8426015c",
   "metadata": {},
   "source": [
    "# Wczytanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "affe9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = '/home2/Kacper_captioning/f30_xception_file_va/'\n",
    "\n",
    "with open(path_to_save + 'all_caption.pkl', 'rb') as fp:\n",
    "    captions_all = pickle.load(fp)\n",
    "with open(path_to_save + 'train_caption.pkl', 'rb') as fp:\n",
    "    train_captions = pickle.load(fp)\n",
    "with open(path_to_save + 'test_caption.pkl', 'rb') as fp:\n",
    "    test_captions = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfacc713",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = utiles.flatten(captions_all)\n",
    "#flat = ' '.join(flat)\n",
    "\n",
    "tokenizer = utiles.TokenizerWrap(texts=flat, num_words=NUM_WORDS)\n",
    "token_start = tokenizer.word_index[MARK_START.strip()]\n",
    "token_end = tokenizer.word_index[MARK_END.strip()]\n",
    "\n",
    "with open(path_to_save + 'train_images.pkl', 'rb') as fp:\n",
    "    train_images = pickle.load(fp)\n",
    "\n",
    "with open(path_to_save + 'test_images.pkl', 'rb') as fp:\n",
    "    test_images = pickle.load(fp)\n",
    "\n",
    "shapes = np.load(train_images[list(train_images.keys())[0]]).shape\n",
    "TRANSFER_VALUE_SIZE = shapes[1]\n",
    "STATE_SIZE = 512\n",
    "UNITS = STATE_SIZE\n",
    "BUFFER_SIZE = 1000\n",
    "VOCAB_SIZE = NUM_WORDS\n",
    "\n",
    "CAPTION_LENGTH = 0\n",
    "for key, value in train_captions.items():\n",
    "    for inner_list in value:\n",
    "        if len(inner_list) > CAPTION_LENGTH:\n",
    "            CAPTION_LENGTH = len(inner_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "095365c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = np.load(train_images[list(train_images.keys())[0]]).shape\n",
    "features_shape = shapes[2] \n",
    "attention_features_shape = shapes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dd2e2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Converted 9724 words (275 misses)\n"
     ]
    }
   ],
   "source": [
    "path_to_glove_file = '/home/mateusz/projects/IM_captioning/glove.6B.300d.txt'\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "\n",
    "num_tokens = VOCAB_SIZE\n",
    "embedding_dim = 300\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= 10_000:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "EMBEDDING_SIZE = embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d452accb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 09:44:44.782006: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-25 09:44:44.784130: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "encoder = utiles_va.CNN_Encoder(EMBEDDING_SIZE)\n",
    "decoder = utiles_va.RNN_Decoder(EMBEDDING_SIZE, UNITS, NUM_WORDS)\n",
    "\n",
    "# embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "# trainable=Fals\n",
    "decoder.embedding.embedding_initializer=tf.keras.initializers.Constant(embedding_matrix)\n",
    "decoder.embeddingtrainable=False\n",
    "\n",
    "start_epochs = 0\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n",
    "\n",
    "checkpoint_path = '/home2/Kacper_captioning/IM_caption_checkpoint_f30_xcpetion_lstm_unit_512_va_emb_glove.keras'\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "#load weights\n",
    "start_epochs = 0\n",
    "try:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e91e8e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a594612",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden_state = decoder.reset_state(batch_size=target.shape[0])\n",
    "    hidden = [hidden_state, hidden_state]\n",
    "    \n",
    "    dec_input = tf.expand_dims([token_start] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss\n",
    "\n",
    "@tf.function\n",
    "def validation_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden_state = decoder.reset_state(batch_size=target.shape[0])\n",
    "    hidden = [hidden_state, hidden_state]\n",
    "\n",
    "    dec_input = tf.expand_dims([token_start] * target.shape[0], 1)\n",
    "\n",
    "    features = encoder(img_tensor)\n",
    "\n",
    "    for i in range(1, target.shape[1]):\n",
    "        # passing the features through the decoder\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "        loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "        # using teacher forcing\n",
    "        dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e73f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "captions_dataset_train = []\n",
    "image_dataset_train = []\n",
    "\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name)\n",
    "    img_tensor = np.reshape(img_tensor, [img_tensor.shape[1], img_tensor.shape[2]])\n",
    "    return img_tensor, cap\n",
    "\n",
    "train_keys = list(train_images.keys())\n",
    "for key in train_keys:\n",
    "    for i in range(len(train_captions[key])):\n",
    "        image_dataset_train.append(train_images[key])\n",
    "        captions_dataset_train.append(train_captions[key][i])\n",
    "\n",
    "tokens_padded = pad_sequences(captions_dataset_train, maxlen=CAPTION_LENGTH, padding='post',truncating='post')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_dataset_train, tokens_padded))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# validation\n",
    "captions_dataset_val = []\n",
    "image_dataset_val = []\n",
    "\n",
    "test_keys = list(test_images.keys())\n",
    "\n",
    "for key in test_keys:\n",
    "    for i in range(len(test_captions[key])):\n",
    "        image_dataset_val.append(test_images[key])\n",
    "        captions_dataset_val.append(test_captions[key][i])\n",
    "\n",
    "tokens_padded_val = pad_sequences(captions_dataset_val, maxlen=CAPTION_LENGTH, padding='post',truncating='post')\n",
    "\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((image_dataset_val, tokens_padded_val))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset_val = dataset_val.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int64]),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset_val = dataset_val.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset_val = dataset_val.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "num_steps_train = len(image_dataset_train) // BATCH_SIZE\n",
    "num_steps_val = len(image_dataset_val) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f5ea1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning\n",
      "Epoch 16 Batch 0 Loss 0.4339\n",
      "Epoch 16 Batch 100 Loss 0.4388\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/framework/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/framework/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/framework/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/framework/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/framework/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/framework/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/framework/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss_plot = []\n",
    "val_loss_plot = []\n",
    "\n",
    "print('Start learning')\n",
    "for epoch in range(15, 50):\n",
    "    total_loss = 0\n",
    "    total_loss_vall = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor.numpy(), target.numpy())\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps_train)\n",
    "    \n",
    "    for (batch_val, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_val_loss, val_loss = validation_step(img_tensor, target)\n",
    "        total_loss_vall += val_loss\n",
    "    val_loss_plot.append(total_loss_vall / num_steps_val)\n",
    "    \n",
    "    # save model after each run\n",
    "    ckpt_manager.save()\n",
    "    \n",
    "    print(f'Epoch {epoch+1} Loss train {total_loss/num_steps_train:.6f} Loss val {total_loss_vall/num_steps_val:.6f}')\n",
    "    #print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fc12cd9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1673908/98850599.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "print(list(np.array(loss_plot)))\n",
    "\n",
    "print(list(np.array(val_loss_plot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bf87e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot =  [ 0.908503, 0.711292, 0.643554, 0.608145, 0.583518, 0.564476,\n",
    "0.548240, 0.533806, 0.520664, 0.508711, 0.498143, 0.488687, 0.479671,\n",
    "0.470429, 0.462272, 0.455087, 0.447597, 0.441342, 0.435186, 0.429486,\n",
    "0.424154, 0.418472, 0.413565, 0.408881, 0.404278, 0.399903, 0.395931,\n",
    "0.391622, 0.387994, 0.383541, 0.379247, 0.376102, 0.373073, 0.369539,\n",
    "0.367210, 0.364955, 0.362524, 0.359903, 0.357207, 0.354768, 0.3521224, \n",
    "0.34993622, 0.34726584, 0.3450418, 0.34313673, 0.34150398, 0.33923087,\n",
    "0.337004, 0.33488223, 0.33315083]\n",
    "\n",
    "val_loss_plot = [2.323149, 1.991009, 1.850668, 1.769556, 1.703860, 1.655447, \n",
    "1.617495, 1.586213, 1.554777,  1.529469, 1.513797, 1.492700, 1.467347,\n",
    "1.450621, 1.437129, 1.426499, 1.417197, 1.402541, 1.384902, 1.372628, \n",
    "1.366134, 1.360891, 1.360441, 1.344014, 1.335475, 1.333687, 1.329918,\n",
    "1.324660, 1.315842, 1.307003, 1.309809, 1.304784, 1.299043, 1.290478,\n",
    "1.279252, 1.274345, 1.265857, 1.257473, 1.254480, 1.256014,1.2550317,\n",
    "1.2465783, 1.2501153, 1.2227339, 1.2134569, 1.2089823, 1.2131509, 1.2168972, \n",
    "1.2127714, 1.2115339]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b76d2254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create figure and axis objects with subplots()\n",
    "fig,ax = plt.subplots(figsize = (7,7))\n",
    "# make a plot\n",
    "ax.plot(loss_plot, color=\"red\", marker=\"o\")\n",
    "# set x-axis label\n",
    "ax.set_xlabel(\"Epochs\", fontsize = 14)\n",
    "# set y-axis label\n",
    "ax.set_ylabel(\"Train Loss\", color=\"red\", fontsize=14)\n",
    "\n",
    "# twin object for two different y-axis on the sample plot\n",
    "ax2=ax.twinx()\n",
    "# make a plot with different y-axis using second axis object\n",
    "ax2.plot(val_loss_plot, color=\"blue\", marker=\"x\")\n",
    "ax2.set_ylabel(\"Validation Loss\", color=\"blue\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fdb5e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Progress: 0.0%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mateusz/anaconda3/envs/framework/lib/python3.7/site-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Progress: 72.0%"
     ]
    }
   ],
   "source": [
    "bleu_1_train = []; bleu_2_train = []; bleu_3_train = []; bleu_4_train = []\n",
    "meteor_train = []\n",
    "rouge_1_train = []; rouge_2_train = []; rouge_L_train = []\n",
    "cider_train = []\n",
    "scorer_rouge = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "chencherry = SmoothingFunction()\n",
    "\n",
    "image_dataset_train_numpy = np.array(image_dataset_train)\n",
    "captions_dataset_train_numpy = np.array(captions_dataset_train)\n",
    "images = set(image_dataset_train)\n",
    "\n",
    "train_prediction = {}\n",
    "counter = 0\n",
    "for element in images:\n",
    "    utiles.print_progress(counter, len(images))\n",
    "\n",
    "    index = np.where(image_dataset_train_numpy == element)\n",
    "    # data\n",
    "    image_val = np.load(element)\n",
    "    reference_captions = captions_dataset_train_numpy[index]\n",
    "    caption_val_raw = []\n",
    "    caption_val_split = []\n",
    "    for el in reference_captions: \n",
    "        cap = tokenizer.tokens_to_string(el[1:-1])\n",
    "        caption_val_raw.append(cap)\n",
    "        caption_val_split.append(cap.split())\n",
    "    \n",
    "    # make prediction\n",
    "    hidden_state = decoder.reset_state(batch_size=1)\n",
    "    hidden = [hidden_state, hidden_state]\n",
    "    \n",
    "    features = encoder(image_val)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index[MARK_END.strip()]], 0)\n",
    "    prediction_caption = []\n",
    "\n",
    "    for i in range(CAPTION_LENGTH):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input,\n",
    "                                                         features,\n",
    "                                                         hidden)\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        predicted_word = tf.compat.as_text(tokenizer.index_to_word[predicted_id])\n",
    "        prediction_caption.append(predicted_word)\n",
    "\n",
    "        if predicted_word == MARK_END:\n",
    "            prediction_caption = prediction_caption[:-1]\n",
    "            break\n",
    "            \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    # calculate matrics\n",
    "    # Calculate BLEU-1 score\n",
    "    bleu_1_train.append(sentence_bleu(caption_val_split, prediction_caption, weights=(1, 0, 0, 0), smoothing_function=chencherry.method1))\n",
    "    # Calculate BLEU-2 score\n",
    "    bleu_2_train.append(sentence_bleu(caption_val_split, prediction_caption, weights=(0.5, 0.5, 0, 0), smoothing_function=chencherry.method1))\n",
    "    # Calculate BLEU-3 score\n",
    "    bleu_3_train.append(sentence_bleu(caption_val_split, prediction_caption, weights=(1/3, 1/3, 1/3, 0), smoothing_function=chencherry.method1))\n",
    "    # Calculate BLEU-4 score\n",
    "    bleu_4_train.append(sentence_bleu(caption_val_split, prediction_caption, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=chencherry.method1))\n",
    "    # calculate Meteor\n",
    "    meteor_train.append(meteor_score(caption_val_split, prediction_caption))\n",
    "    # rouge score\n",
    "    for el in caption_val_raw:\n",
    "        rouge_scores = scorer_rouge.score(el, ' '.join(prediction_caption))\n",
    "        rouge_1_train.append(rouge_scores['rouge1'].fmeasure)\n",
    "        rouge_2_train.append(rouge_scores['rouge2'].fmeasure)\n",
    "        rouge_L_train.append(rouge_scores['rougeL'].fmeasure)\n",
    "\n",
    "    counter += 1\n",
    "    key_img = element.split('/')[-1][:-4]\n",
    "    train_prediction[key_img] =  ' '.join(prediction_caption)\n",
    "\n",
    "print(\"#\"*20)\n",
    "print(\"\\tMetrics train set\")\n",
    "print(\"BLEU-1: \", np.round(np.mean(bleu_1_train),2), '+-', np.round(np.std(bleu_1_train),2))\n",
    "print(\"BLEU-2: \", np.round(np.mean(bleu_2_train),2), '+-', np.round(np.std(bleu_2_train),2))\n",
    "print(\"BLEU-3: \", np.round(np.mean(bleu_3_train),2), '+-', np.round(np.std(bleu_3_train),2))\n",
    "print(\"BLEU-4: \", np.round(np.mean(bleu_4_train),2), '+-', np.round(np.std(bleu_4_train),2))\n",
    "print(\"METEOR: \", np.round(np.mean(meteor_train),2), '+-', np.round(np.std(meteor_train),2))\n",
    "print(\"ROUGE-1: \", np.round(np.mean(rouge_1_train),2), '+-', np.round(np.std(rouge_1_train),2))\n",
    "print(\"ROUGE-2: \", np.round(np.mean(rouge_2_train),2), '+-', np.round(np.std(rouge_2_train),2))\n",
    "print(\"ROUGE-L: \", np.round(np.mean(rouge_L_train),2), '+-', np.round(np.std(rouge_L_train),2))\n",
    "\n",
    "# metrics calculation for validation set\n",
    "cider_val = []\n",
    "bleu_1_val = []; bleu_2_val = []; bleu_3_val = []; bleu_4_val = []\n",
    "meteor_val = []\n",
    "rouge_1_val = []; rouge_2_val = []; rouge_L_val = []\n",
    "scorer_rouge = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "chencherry = SmoothingFunction()\n",
    "\n",
    "image_dataset_val_numpy = np.array(image_dataset_val)\n",
    "captions_dataset_val_numpy = np.array(captions_dataset_val)\n",
    "images = set(image_dataset_val)\n",
    "\n",
    "val_prediction = {}\n",
    "counter = 0\n",
    "for element in images:\n",
    "    utiles.print_progress(counter, len(images))\n",
    "\n",
    "    index = np.where(image_dataset_val_numpy == element)\n",
    "    # data\n",
    "    image_val = np.load(element)\n",
    "    reference_captions = captions_dataset_val_numpy[index]\n",
    "    caption_val_raw = []\n",
    "    caption_val_split = []\n",
    "    for el in reference_captions: \n",
    "        cap = tokenizer.tokens_to_string(el[1:-1])\n",
    "        caption_val_raw.append(cap)\n",
    "        caption_val_split.append(cap.split())\n",
    "    \n",
    "    # make prediction\n",
    "    hidden_state = decoder.reset_state(batch_size=1)\n",
    "    hidden = [hidden_state, hidden_state]\n",
    "    \n",
    "    features = encoder(image_val)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index[MARK_END.strip()]], 0)\n",
    "    prediction_caption = []\n",
    "\n",
    "    for i in range(CAPTION_LENGTH):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input,\n",
    "                                                         features,\n",
    "                                                         hidden)\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        predicted_word = tf.compat.as_text(tokenizer.index_to_word[predicted_id])\n",
    "        prediction_caption.append(predicted_word)\n",
    "\n",
    "        if predicted_word == MARK_END:\n",
    "            prediction_caption = prediction_caption[:-1]\n",
    "            break\n",
    "            \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    # calculate matrics\n",
    "    # Calculate BLEU-1 score\n",
    "    bleu_1_val.append(sentence_bleu(caption_val_split, prediction_caption, weights=(1, 0, 0, 0), smoothing_function=chencherry.method1))\n",
    "    # Calculate BLEU-2 score\n",
    "    bleu_2_val.append(sentence_bleu(caption_val_split, prediction_caption, weights=(0.5, 0.5, 0, 0), smoothing_function=chencherry.method1))\n",
    "    # Calculate BLEU-3 score\n",
    "    bleu_3_val.append(sentence_bleu(caption_val_split, prediction_caption, weights=(1/3, 1/3, 1/3, 0), smoothing_function=chencherry.method1))\n",
    "    # Calculate BLEU-4 score\n",
    "    bleu_4_val.append(sentence_bleu(caption_val_split, prediction_caption, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=chencherry.method1))\n",
    "    # calculate Meteor\n",
    "    meteor_val.append(meteor_score(caption_val_split, prediction_caption))\n",
    "    # rouge score\n",
    "    for el in caption_val_raw:\n",
    "        rouge_scores = scorer_rouge.score(el, ' '.join(prediction_caption))\n",
    "        rouge_1_val.append(rouge_scores['rouge1'].fmeasure)\n",
    "        rouge_2_val.append(rouge_scores['rouge2'].fmeasure)\n",
    "        rouge_L_val.append(rouge_scores['rougeL'].fmeasure)\n",
    "    \n",
    "    key_img = element.split('/')[-1][:-4]\n",
    "    val_prediction[key_img] =  ' '.join(prediction_caption)\n",
    "    counter +=1\n",
    " \n",
    "print(\"#\"*20)\n",
    "print(\"\\tMetrics val set\")\n",
    "print(\"BLEU-1: \", np.round(np.mean(bleu_1_val),2), '+-', np.round(np.std(bleu_1_val),2))\n",
    "print(\"BLEU-2: \", np.round(np.mean(bleu_2_val),2), '+-', np.round(np.std(bleu_2_val),2))\n",
    "print(\"BLEU-3: \", np.round(np.mean(bleu_3_val),2), '+-', np.round(np.std(bleu_3_val),2))\n",
    "print(\"BLEU-4: \", np.round(np.mean(bleu_4_val),2), '+-', np.round(np.std(bleu_4_val),2))\n",
    "print(\"METEOR: \", np.round(np.mean(meteor_val),2), '+-', np.round(np.std(meteor_val),2))\n",
    "print(\"ROUGE-1: \", np.round(np.mean(rouge_1_val),2), '+-', np.round(np.std(rouge_1_val),2))\n",
    "print(\"ROUGE-2: \", np.round(np.mean(rouge_2_val),2), '+-', np.round(np.std(rouge_2_val),2))\n",
    "print(\"ROUGE-L: \", np.round(np.mean(rouge_L_val),2), '+-', np.round(np.std(rouge_L_val),2))\n",
    "                                          \n",
    "path_to_save = '/home2/Kacper_captioning/f30_xception_file_va/'\n",
    "\n",
    "with open(path_to_save + 'train_predictions_units_512_lstm.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_prediction, fp)\n",
    "with open(path_to_save + 'test_predictions_units_512_lstm.pkl', 'wb') as fp:\n",
    "    pickle.dump(val_prediction, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "626e04eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "\tMetrics train set\n",
      "BLEU-1:  0.35 +- 0.2\n",
      "BLEU-2:  0.18 +- 0.17\n",
      "BLEU-3:  0.11 +- 0.13\n",
      "BLEU-4:  0.07 +- 0.09\n",
      "METEOR:  0.23 +- 0.16\n",
      "ROUGE-1:  0.17 +- 0.15\n",
      "ROUGE-2:  0.04 +- 0.09\n",
      "ROUGE-L:  0.16 +- 0.14\n",
      "####################\n",
      "\tMetrics val set\n",
      "BLEU-1:  0.27 +- 0.17\n",
      "BLEU-2:  0.11 +- 0.12\n",
      "BLEU-3:  0.06 +- 0.07\n",
      "BLEU-4:  0.04 +- 0.05\n",
      "METEOR:  0.16 +- 0.13\n",
      "ROUGE-1:  0.13 +- 0.12\n",
      "ROUGE-2:  0.02 +- 0.06\n",
      "ROUGE-L:  0.12 +- 0.11\n"
     ]
    }
   ],
   "source": [
    "print(\"#\"*20)\n",
    "print(\"\\tMetrics train set\")\n",
    "print(\"BLEU-1: \", np.round(np.mean(bleu_1_train),2), '+-', np.round(np.std(bleu_1_train),2))\n",
    "print(\"BLEU-2: \", np.round(np.mean(bleu_2_train),2), '+-', np.round(np.std(bleu_2_train),2))\n",
    "print(\"BLEU-3: \", np.round(np.mean(bleu_3_train),2), '+-', np.round(np.std(bleu_3_train),2))\n",
    "print(\"BLEU-4: \", np.round(np.mean(bleu_4_train),2), '+-', np.round(np.std(bleu_4_train),2))\n",
    "print(\"METEOR: \", np.round(np.mean(meteor_train),2), '+-', np.round(np.std(meteor_train),2))\n",
    "print(\"ROUGE-1: \", np.round(np.mean(rouge_1_train),2), '+-', np.round(np.std(rouge_1_train),2))\n",
    "print(\"ROUGE-2: \", np.round(np.mean(rouge_2_train),2), '+-', np.round(np.std(rouge_2_train),2))\n",
    "print(\"ROUGE-L: \", np.round(np.mean(rouge_L_train),2), '+-', np.round(np.std(rouge_L_train),2))\n",
    "print(\"#\"*20)\n",
    "\n",
    "print(\"\\tMetrics val set\")\n",
    "print(\"BLEU-1: \", np.round(np.mean(bleu_1_val),2), '+-', np.round(np.std(bleu_1_val),2))\n",
    "print(\"BLEU-2: \", np.round(np.mean(bleu_2_val),2), '+-', np.round(np.std(bleu_2_val),2))\n",
    "print(\"BLEU-3: \", np.round(np.mean(bleu_3_val),2), '+-', np.round(np.std(bleu_3_val),2))\n",
    "print(\"BLEU-4: \", np.round(np.mean(bleu_4_val),2), '+-', np.round(np.std(bleu_4_val),2))\n",
    "print(\"METEOR: \", np.round(np.mean(meteor_val),2), '+-', np.round(np.std(meteor_val),2))\n",
    "print(\"ROUGE-1: \", np.round(np.mean(rouge_1_val),2), '+-', np.round(np.std(rouge_1_val),2))\n",
    "print(\"ROUGE-2: \", np.round(np.mean(rouge_2_val),2), '+-', np.round(np.std(rouge_2_val),2))\n",
    "print(\"ROUGE-L: \", np.round(np.mean(rouge_L_val),2), '+-', np.round(np.std(rouge_L_val),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bed66056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Progress: 100.0%####################\n",
      "\tMetrics val set\n",
      "BLEU-1:  0.28 +- 0.2\n",
      "BLEU-2:  0.13 +- 0.15\n",
      "BLEU-3:  0.07 +- 0.1\n",
      "BLEU-4:  0.05 +- 0.07\n",
      "METEOR:  0.19 +- 0.16\n",
      "ROUGE-1:  0.15 +- 0.14\n",
      "ROUGE-2:  0.02 +- 0.07\n",
      "ROUGE-L:  0.14 +- 0.14\n"
     ]
    }
   ],
   "source": [
    "val_prediction = {}\n",
    "counter = 0\n",
    "for element in images:\n",
    "    utiles.print_progress(counter, len(images))\n",
    "\n",
    "    index = np.where(image_dataset_val_numpy == element)\n",
    "    # data\n",
    "    image_val = np.load(element)\n",
    "    reference_captions = captions_dataset_val_numpy[index]\n",
    "    caption_val_raw = []\n",
    "    caption_val_split = []\n",
    "    for el in reference_captions: \n",
    "        cap = tokenizer.tokens_to_string(el[1:-1])\n",
    "        caption_val_raw.append(cap)\n",
    "        caption_val_split.append(cap.split())\n",
    "    \n",
    "    # make prediction\n",
    "    hidden_state = decoder.reset_state(batch_size=1)\n",
    "    hidden = [hidden_state, hidden_state]\n",
    "    \n",
    "    features = encoder(image_val)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index[MARK_END.strip()]], 0)\n",
    "    prediction_caption = []\n",
    "\n",
    "    for i in range(CAPTION_LENGTH):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input,\n",
    "                                                         features,\n",
    "                                                         hidden)\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        predicted_word = tf.compat.as_text(tokenizer.index_to_word[predicted_id])\n",
    "        prediction_caption.append(predicted_word)\n",
    "\n",
    "        if predicted_word == MARK_END:\n",
    "            prediction_caption = prediction_caption[:-1]\n",
    "            break\n",
    "            \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    # calculate matrics\n",
    "    # Calculate BLEU-1 score\n",
    "    bleu_1_val.append(sentence_bleu(caption_val_split, prediction_caption, weights=(1, 0, 0, 0), smoothing_function=chencherry.method1))\n",
    "    # Calculate BLEU-2 score\n",
    "    bleu_2_val.append(sentence_bleu(caption_val_split, prediction_caption, weights=(0.5, 0.5, 0, 0), smoothing_function=chencherry.method1))\n",
    "    # Calculate BLEU-3 score\n",
    "    bleu_3_val.append(sentence_bleu(caption_val_split, prediction_caption, weights=(1/3, 1/3, 1/3, 0), smoothing_function=chencherry.method1))\n",
    "    # Calculate BLEU-4 score\n",
    "    bleu_4_val.append(sentence_bleu(caption_val_split, prediction_caption, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=chencherry.method1))\n",
    "    # calculate Meteor\n",
    "    meteor_val.append(meteor_score(caption_val_split, prediction_caption))\n",
    "    # rouge score\n",
    "    for el in caption_val_raw:\n",
    "        rouge_scores = scorer_rouge.score(el, ' '.join(prediction_caption))\n",
    "        rouge_1_val.append(rouge_scores['rouge1'].fmeasure)\n",
    "        rouge_2_val.append(rouge_scores['rouge2'].fmeasure)\n",
    "        rouge_L_val.append(rouge_scores['rougeL'].fmeasure)\n",
    "    \n",
    "    key_img = element.split('/')[-1][:-4]\n",
    "    val_prediction[key_img] =  ' '.join(prediction_caption)\n",
    "    counter +=1\n",
    " \n",
    "print(\"#\"*20)\n",
    "print(\"\\tMetrics val set\")\n",
    "print(\"BLEU-1: \", np.round(np.mean(bleu_1_val),2), '+-', np.round(np.std(bleu_1_val),2))\n",
    "print(\"BLEU-2: \", np.round(np.mean(bleu_2_val),2), '+-', np.round(np.std(bleu_2_val),2))\n",
    "print(\"BLEU-3: \", np.round(np.mean(bleu_3_val),2), '+-', np.round(np.std(bleu_3_val),2))\n",
    "print(\"BLEU-4: \", np.round(np.mean(bleu_4_val),2), '+-', np.round(np.std(bleu_4_val),2))\n",
    "print(\"METEOR: \", np.round(np.mean(meteor_val),2), '+-', np.round(np.std(meteor_val),2))\n",
    "print(\"ROUGE-1: \", np.round(np.mean(rouge_1_val),2), '+-', np.round(np.std(rouge_1_val),2))\n",
    "print(\"ROUGE-2: \", np.round(np.mean(rouge_2_val),2), '+-', np.round(np.std(rouge_2_val),2))\n",
    "print(\"ROUGE-L: \", np.round(np.mean(rouge_L_val),2), '+-', np.round(np.std(rouge_L_val),2))\n",
    "                                          \n",
    "path_to_save = '/home2/Kacper_captioning/f8_xception_file_va/'\n",
    "\n",
    "with open(path_to_save + 'train_predictions_lstm.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_prediction, fp)\n",
    "with open(path_to_save + 'test_predictions_lstn.pkl', 'wb') as fp:\n",
    "    pickle.dump(val_prediction, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ace5b203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9255759d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3bae84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
