{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2911b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import utiles \n",
    "import utiles_va\n",
    "from os import listdir\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Embedding, LSTM, add, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d907a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_COCO_TOKENS = '/home2/data/images/coco2017/annotations'\n",
    "PATH_FILCKR30k_TOKENS = '/home2/Kacper_captioning/flicker_30k/annotations/captions.txt'\n",
    "PATH_COCO_IMAGES = '/home2/data/images/coco2017'\n",
    "PATH_FILCKR30k_IMAGES = '/home2/data/images/flickr30k/Images/flickr30k_images'\n",
    "\n",
    "PATH_FLICKR8k_IMAGES = '/home2/data/images/flickr8k/Images'\n",
    "PATH_FILCKR8k_TOKENS = '/home2/data/images/flickr8k/captions.txt'\n",
    "\n",
    "NUM_WORDS = 10_000\n",
    "MARK_START = 'ssss'\n",
    "MARK_END = 'eeee'\n",
    "SEED = 3\n",
    "BATCH_SIZE = 128#128 #256\n",
    "EMBEDDING_SIZE = 256\n",
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d771502",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = '/home2/Kacper_captioning/f8_xception_file_va/'\n",
    "\n",
    "with open(path_to_save + 'all_caption.pkl', 'rb') as fp:\n",
    "    captions_all = pickle.load(fp)\n",
    "with open(path_to_save + 'train_caption.pkl', 'rb') as fp:\n",
    "    train_captions = pickle.load(fp)\n",
    "with open(path_to_save + 'test_caption.pkl', 'rb') as fp:\n",
    "    test_captions = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aa2c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = utiles.flatten(captions_all)\n",
    "#flat = ' '.join(flat)\n",
    "\n",
    "tokenizer = utiles.TokenizerWrap(texts=flat, num_words=NUM_WORDS)\n",
    "token_start = tokenizer.word_index[MARK_START.strip()]\n",
    "token_end = tokenizer.word_index[MARK_END.strip()]\n",
    "\n",
    "with open(path_to_save + 'train_images.pkl', 'rb') as fp:\n",
    "    train_images = pickle.load(fp)\n",
    "\n",
    "with open(path_to_save + 'test_images.pkl', 'rb') as fp:\n",
    "    test_images = pickle.load(fp)\n",
    "\n",
    "shapes = np.load(train_images[list(train_images.keys())[0]]).shape\n",
    "TRANSFER_VALUE_SIZE = shapes[1]\n",
    "STATE_SIZE = 512\n",
    "UNITS = STATE_SIZE\n",
    "BUFFER_SIZE = 1000\n",
    "VOCAB_SIZE = NUM_WORDS\n",
    "\n",
    "CAPTION_LENGTH = 0\n",
    "for key, value in train_captions.items():\n",
    "    for inner_list in value:\n",
    "        if len(inner_list) > CAPTION_LENGTH:\n",
    "            CAPTION_LENGTH = len(inner_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000b4f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = np.load(train_images[list(train_images.keys())[0]]).shape\n",
    "features_shape = shapes[2] \n",
    "attention_features_shape = shapes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a536c54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-12 22:52:32.884826: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-12 22:52:32.890057: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "encoder = utiles_va.CNN_Encoder(EMBEDDING_SIZE)\n",
    "decoder = utiles_va.RNN_Decoder(EMBEDDING_SIZE, UNITS, NUM_WORDS)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n",
    "\n",
    "checkpoint_path = '/home2/Kacper_captioning/IM_caption_checkpoint_f8_xception_gru_va_units_512_back_100.keras'\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "#load weights\n",
    "start_epochs = 0\n",
    "try:\n",
    "    #start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f57b23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([token_start] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss\n",
    "\n",
    "@tf.function\n",
    "def validation_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([token_start] * target.shape[0], 1)\n",
    "\n",
    "    features = encoder(img_tensor)\n",
    "\n",
    "    for i in range(1, target.shape[1]):\n",
    "        # passing the features through the decoder\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "        loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "        # using teacher forcing\n",
    "        dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e8f4fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "captions_dataset_train = []\n",
    "image_dataset_train = []\n",
    "\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name)\n",
    "    img_tensor = np.reshape(img_tensor, [img_tensor.shape[1], img_tensor.shape[2]])\n",
    "    return img_tensor, cap\n",
    "\n",
    "train_keys = list(train_images.keys())\n",
    "for key in train_keys:\n",
    "    for i in range(len(train_captions[key])):\n",
    "        image_dataset_train.append(train_images[key])\n",
    "        captions_dataset_train.append(train_captions[key][i])\n",
    "\n",
    "tokens_padded = pad_sequences(captions_dataset_train, maxlen=CAPTION_LENGTH, padding='post',truncating='post')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_dataset_train, tokens_padded))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# validation\n",
    "captions_dataset_val = []\n",
    "image_dataset_val = []\n",
    "\n",
    "test_keys = list(test_images.keys())\n",
    "\n",
    "for key in test_keys:\n",
    "    for i in range(len(test_captions[key])):\n",
    "        image_dataset_val.append(test_images[key])\n",
    "        captions_dataset_val.append(test_captions[key][i])\n",
    "\n",
    "tokens_padded_val = pad_sequences(captions_dataset_val, maxlen=CAPTION_LENGTH, padding='post',truncating='post')\n",
    "\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((image_dataset_val, tokens_padded_val))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset_val = dataset_val.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int64]),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset_val = dataset_val.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset_val = dataset_val.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "num_steps_train = len(image_dataset_train) // BATCH_SIZE\n",
    "num_steps_val = len(image_dataset_val) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20977006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning\n",
      "Epoch 1 Batch 0 Loss 2.6734\n",
      "Epoch 1 Batch 100 Loss 1.5894\n",
      "Epoch 1 Batch 200 Loss 1.4331\n",
      "Epoch 1 Loss train 1.569062 Loss val 3.962380\n",
      "Epoch 2 Batch 0 Loss 1.5091\n",
      "Epoch 2 Batch 100 Loss 1.3078\n",
      "Epoch 2 Batch 200 Loss 1.1282\n",
      "Epoch 2 Loss train 1.207963 Loss val 3.319160\n",
      "Epoch 3 Batch 0 Loss 1.0698\n",
      "Epoch 3 Batch 100 Loss 1.0154\n",
      "Epoch 3 Batch 200 Loss 1.0941\n",
      "Epoch 3 Loss train 1.061322 Loss val 2.982822\n",
      "Epoch 4 Batch 0 Loss 1.0349\n",
      "Epoch 4 Batch 100 Loss 1.0479\n",
      "Epoch 4 Batch 200 Loss 0.9324\n",
      "Epoch 4 Loss train 0.972012 Loss val 2.760356\n",
      "Epoch 5 Batch 0 Loss 0.9290\n",
      "Epoch 5 Batch 100 Loss 0.8858\n",
      "Epoch 5 Batch 200 Loss 0.9012\n",
      "Epoch 5 Loss train 0.902266 Loss val 2.588561\n",
      "Epoch 6 Batch 0 Loss 0.9135\n",
      "Epoch 6 Batch 100 Loss 0.7951\n",
      "Epoch 6 Batch 200 Loss 0.8739\n",
      "Epoch 6 Loss train 0.842883 Loss val 2.439806\n",
      "Epoch 7 Batch 0 Loss 0.8128\n",
      "Epoch 7 Batch 100 Loss 0.7693\n",
      "Epoch 7 Batch 200 Loss 0.8065\n",
      "Epoch 7 Loss train 0.793618 Loss val 2.346324\n",
      "Epoch 8 Batch 0 Loss 0.8204\n",
      "Epoch 8 Batch 100 Loss 0.8219\n",
      "Epoch 8 Batch 200 Loss 0.7083\n",
      "Epoch 8 Loss train 0.752319 Loss val 2.235497\n",
      "Epoch 9 Batch 0 Loss 0.6921\n",
      "Epoch 9 Batch 100 Loss 0.6682\n",
      "Epoch 9 Batch 200 Loss 0.7251\n",
      "Epoch 9 Loss train 0.712371 Loss val 2.117076\n",
      "Epoch 10 Batch 0 Loss 0.7129\n",
      "Epoch 10 Batch 100 Loss 0.6671\n",
      "Epoch 10 Batch 200 Loss 0.7032\n",
      "Epoch 10 Loss train 0.673821 Loss val 2.049749\n",
      "Epoch 11 Batch 0 Loss 0.6437\n",
      "Epoch 11 Batch 100 Loss 0.5946\n",
      "Epoch 11 Batch 200 Loss 0.6113\n",
      "Epoch 11 Loss train 0.639539 Loss val 1.984599\n",
      "Epoch 12 Batch 0 Loss 0.6433\n",
      "Epoch 12 Batch 100 Loss 0.5855\n",
      "Epoch 12 Batch 200 Loss 0.5939\n",
      "Epoch 12 Loss train 0.610642 Loss val 1.988447\n",
      "Epoch 13 Batch 0 Loss 0.6552\n",
      "Epoch 13 Batch 100 Loss 0.5944\n",
      "Epoch 13 Batch 200 Loss 0.5965\n",
      "Epoch 13 Loss train 0.588583 Loss val 1.901863\n",
      "Epoch 14 Batch 0 Loss 0.6058\n",
      "Epoch 14 Batch 100 Loss 0.5474\n",
      "Epoch 14 Batch 200 Loss 0.5348\n",
      "Epoch 14 Loss train 0.566816 Loss val 1.831753\n",
      "Epoch 15 Batch 0 Loss 0.5716\n",
      "Epoch 15 Batch 100 Loss 0.4945\n",
      "Epoch 15 Batch 200 Loss 0.5639\n",
      "Epoch 15 Loss train 0.540693 Loss val 1.768046\n",
      "Epoch 16 Batch 0 Loss 0.5136\n",
      "Epoch 16 Batch 100 Loss 0.5209\n",
      "Epoch 16 Batch 200 Loss 0.4837\n",
      "Epoch 16 Loss train 0.519028 Loss val 1.765894\n",
      "Epoch 17 Batch 0 Loss 0.5709\n",
      "Epoch 17 Batch 100 Loss 0.5361\n",
      "Epoch 17 Batch 200 Loss 0.4730\n",
      "Epoch 17 Loss train 0.507619 Loss val 1.703692\n",
      "Epoch 18 Batch 0 Loss 0.5049\n",
      "Epoch 18 Batch 100 Loss 0.4792\n",
      "Epoch 18 Batch 200 Loss 0.4873\n",
      "Epoch 18 Loss train 0.481559 Loss val 1.638495\n",
      "Epoch 19 Batch 0 Loss 0.4645\n",
      "Epoch 19 Batch 100 Loss 0.4565\n",
      "Epoch 19 Batch 200 Loss 0.4330\n",
      "Epoch 19 Loss train 0.462934 Loss val 1.604164\n",
      "Epoch 20 Batch 0 Loss 0.4512\n",
      "Epoch 20 Batch 100 Loss 0.4715\n",
      "Epoch 20 Batch 200 Loss 0.4101\n",
      "Epoch 20 Loss train 0.445173 Loss val 1.614792\n",
      "Epoch 21 Batch 0 Loss 0.4476\n",
      "Epoch 21 Batch 100 Loss 0.4262\n",
      "Epoch 21 Batch 200 Loss 0.4195\n",
      "Epoch 21 Loss train 0.431043 Loss val 1.596000\n",
      "Epoch 22 Batch 0 Loss 0.4134\n",
      "Epoch 22 Batch 100 Loss 0.3821\n",
      "Epoch 22 Batch 200 Loss 0.4106\n",
      "Epoch 22 Loss train 0.418010 Loss val 1.562026\n",
      "Epoch 23 Batch 0 Loss 0.4205\n",
      "Epoch 23 Batch 100 Loss 0.4100\n",
      "Epoch 23 Batch 200 Loss 0.3864\n",
      "Epoch 23 Loss train 0.408729 Loss val 1.551900\n",
      "Epoch 24 Batch 0 Loss 0.4374\n",
      "Epoch 24 Batch 100 Loss 0.3947\n",
      "Epoch 24 Batch 200 Loss 0.3666\n",
      "Epoch 24 Loss train 0.395789 Loss val 1.528616\n",
      "Epoch 25 Batch 0 Loss 0.4067\n",
      "Epoch 25 Batch 100 Loss 0.3718\n",
      "Epoch 25 Batch 200 Loss 0.3473\n",
      "Epoch 25 Loss train 0.382695 Loss val 1.496509\n",
      "Epoch 26 Batch 0 Loss 0.3810\n",
      "Epoch 26 Batch 100 Loss 0.3607\n",
      "Epoch 26 Batch 200 Loss 0.3423\n",
      "Epoch 26 Loss train 0.370346 Loss val 1.502054\n",
      "Epoch 27 Batch 0 Loss 0.3661\n",
      "Epoch 27 Batch 100 Loss 0.3675\n",
      "Epoch 27 Batch 200 Loss 0.3547\n",
      "Epoch 27 Loss train 0.363388 Loss val 1.452746\n",
      "Epoch 28 Batch 0 Loss 0.3651\n",
      "Epoch 28 Batch 100 Loss 0.3618\n",
      "Epoch 28 Batch 200 Loss 0.3297\n",
      "Epoch 28 Loss train 0.351343 Loss val 1.450398\n",
      "Epoch 29 Batch 0 Loss 0.3827\n",
      "Epoch 29 Batch 100 Loss 0.3523\n",
      "Epoch 29 Batch 200 Loss 0.3511\n",
      "Epoch 29 Loss train 0.344955 Loss val 1.410838\n",
      "Epoch 30 Batch 0 Loss 0.3243\n",
      "Epoch 30 Batch 100 Loss 0.3268\n",
      "Epoch 30 Batch 200 Loss 0.3190\n",
      "Epoch 30 Loss train 0.341003 Loss val 1.443517\n",
      "Epoch 31 Batch 0 Loss 0.3470\n",
      "Epoch 31 Batch 100 Loss 0.3083\n",
      "Epoch 31 Batch 200 Loss 0.3220\n",
      "Epoch 31 Loss train 0.331731 Loss val 1.407200\n",
      "Epoch 32 Batch 0 Loss 0.3425\n",
      "Epoch 32 Batch 100 Loss 0.3139\n",
      "Epoch 32 Batch 200 Loss 0.2959\n",
      "Epoch 32 Loss train 0.326350 Loss val 1.368517\n",
      "Epoch 33 Batch 0 Loss 0.2987\n",
      "Epoch 33 Batch 100 Loss 0.2927\n",
      "Epoch 33 Batch 200 Loss 0.3184\n",
      "Epoch 33 Loss train 0.311347 Loss val 1.381461\n",
      "Epoch 34 Batch 0 Loss 0.3063\n",
      "Epoch 34 Batch 100 Loss 0.3180\n",
      "Epoch 34 Batch 200 Loss 0.3095\n",
      "Epoch 34 Loss train 0.309899 Loss val 1.405603\n",
      "Epoch 35 Batch 0 Loss 0.2830\n",
      "Epoch 35 Batch 100 Loss 0.3131\n",
      "Epoch 35 Batch 200 Loss 0.2774\n",
      "Epoch 35 Loss train 0.298535 Loss val 1.373818\n",
      "Epoch 36 Batch 0 Loss 0.3231\n",
      "Epoch 36 Batch 100 Loss 0.2831\n",
      "Epoch 36 Batch 200 Loss 0.2972\n",
      "Epoch 36 Loss train 0.295420 Loss val 1.348743\n",
      "Epoch 37 Batch 0 Loss 0.2925\n",
      "Epoch 37 Batch 100 Loss 0.2905\n",
      "Epoch 37 Batch 200 Loss 0.2615\n",
      "Epoch 37 Loss train 0.285356 Loss val 1.344308\n",
      "Epoch 38 Batch 0 Loss 0.2861\n",
      "Epoch 38 Batch 100 Loss 0.2674\n",
      "Epoch 38 Batch 200 Loss 0.2707\n",
      "Epoch 38 Loss train 0.283806 Loss val 1.339930\n",
      "Epoch 39 Batch 0 Loss 0.2852\n",
      "Epoch 39 Batch 100 Loss 0.2629\n",
      "Epoch 39 Batch 200 Loss 0.2716\n",
      "Epoch 39 Loss train 0.273990 Loss val 1.334535\n",
      "Epoch 40 Batch 0 Loss 0.2802\n",
      "Epoch 40 Batch 100 Loss 0.2677\n",
      "Epoch 40 Batch 200 Loss 0.2880\n",
      "Epoch 40 Loss train 0.276485 Loss val 1.310094\n",
      "Epoch 41 Batch 0 Loss 0.2550\n",
      "Epoch 41 Batch 100 Loss 0.2401\n",
      "Epoch 41 Batch 200 Loss 0.2420\n",
      "Epoch 41 Loss train 0.267223 Loss val 1.343769\n",
      "Epoch 42 Batch 0 Loss 0.2714\n",
      "Epoch 42 Batch 100 Loss 0.2546\n",
      "Epoch 42 Batch 200 Loss 0.2536\n",
      "Epoch 42 Loss train 0.267159 Loss val 1.351836\n",
      "Epoch 43 Batch 0 Loss 0.2767\n",
      "Epoch 43 Batch 100 Loss 0.2502\n",
      "Epoch 43 Batch 200 Loss 0.2373\n",
      "Epoch 43 Loss train 0.261002 Loss val 1.301402\n",
      "Epoch 44 Batch 0 Loss 0.2804\n",
      "Epoch 44 Batch 100 Loss 0.2503\n",
      "Epoch 44 Batch 200 Loss 0.2245\n",
      "Epoch 44 Loss train 0.253912 Loss val 1.287805\n",
      "Epoch 45 Batch 0 Loss 0.2519\n",
      "Epoch 45 Batch 100 Loss 0.2400\n",
      "Epoch 45 Batch 200 Loss 0.2573\n",
      "Epoch 45 Loss train 0.245017 Loss val 1.319179\n",
      "Epoch 46 Batch 0 Loss 0.2617\n",
      "Epoch 46 Batch 100 Loss 0.2333\n",
      "Epoch 46 Batch 200 Loss 0.2286\n",
      "Epoch 46 Loss train 0.248333 Loss val 1.315763\n",
      "Epoch 47 Batch 0 Loss 0.2576\n",
      "Epoch 47 Batch 100 Loss 0.2383\n",
      "Epoch 47 Batch 200 Loss 0.2556\n",
      "Epoch 47 Loss train 0.245676 Loss val 1.273982\n",
      "Epoch 48 Batch 0 Loss 0.2393\n",
      "Epoch 48 Batch 100 Loss 0.2048\n",
      "Epoch 48 Batch 200 Loss 0.2263\n",
      "Epoch 48 Loss train 0.240626 Loss val 1.300930\n",
      "Epoch 49 Batch 0 Loss 0.2647\n",
      "Epoch 49 Batch 100 Loss 0.2134\n",
      "Epoch 49 Batch 200 Loss 0.2237\n",
      "Epoch 49 Loss train 0.238705 Loss val 1.284250\n",
      "Epoch 50 Batch 0 Loss 0.2330\n",
      "Epoch 50 Batch 100 Loss 0.2261\n",
      "Epoch 50 Batch 200 Loss 0.2347\n",
      "Epoch 50 Loss train 0.232963 Loss val 1.245038\n",
      "Epoch 51 Batch 0 Loss 0.2262\n",
      "Epoch 51 Batch 100 Loss 0.2333\n",
      "Epoch 51 Batch 200 Loss 0.2238\n",
      "Epoch 51 Loss train 0.233836 Loss val 1.290809\n",
      "Epoch 52 Batch 0 Loss 0.2273\n",
      "Epoch 52 Batch 100 Loss 0.2112\n",
      "Epoch 52 Batch 200 Loss 0.2113\n",
      "Epoch 52 Loss train 0.230734 Loss val 1.252313\n",
      "Epoch 53 Batch 0 Loss 0.2226\n",
      "Epoch 53 Batch 100 Loss 0.2109\n",
      "Epoch 53 Batch 200 Loss 0.1963\n",
      "Epoch 53 Loss train 0.226151 Loss val 1.297105\n",
      "Epoch 54 Batch 0 Loss 0.2368\n",
      "Epoch 54 Batch 100 Loss 0.2221\n",
      "Epoch 54 Batch 200 Loss 0.2398\n",
      "Epoch 54 Loss train 0.235838 Loss val 1.259955\n",
      "Epoch 55 Batch 0 Loss 0.2410\n",
      "Epoch 55 Batch 100 Loss 0.2223\n",
      "Epoch 55 Batch 200 Loss 0.2135\n",
      "Epoch 55 Loss train 0.228136 Loss val 1.244620\n",
      "Epoch 56 Batch 0 Loss 0.2114\n",
      "Epoch 56 Batch 100 Loss 0.2109\n",
      "Epoch 56 Batch 200 Loss 0.1952\n",
      "Epoch 56 Loss train 0.227405 Loss val 1.314497\n",
      "Epoch 57 Batch 0 Loss 0.2499\n",
      "Epoch 57 Batch 100 Loss 0.2141\n",
      "Epoch 57 Batch 200 Loss 0.1957\n",
      "Epoch 57 Loss train 0.228389 Loss val 1.241291\n",
      "Epoch 58 Batch 0 Loss 0.2194\n",
      "Epoch 58 Batch 100 Loss 0.2118\n",
      "Epoch 58 Batch 200 Loss 0.2318\n",
      "Epoch 58 Loss train 0.221991 Loss val 1.228694\n",
      "Epoch 59 Batch 0 Loss 0.2001\n",
      "Epoch 59 Batch 100 Loss 0.1977\n",
      "Epoch 59 Batch 200 Loss 0.2002\n",
      "Epoch 59 Loss train 0.214390 Loss val 1.247521\n",
      "Epoch 60 Batch 0 Loss 0.1911\n",
      "Epoch 60 Batch 100 Loss 0.1999\n",
      "Epoch 60 Batch 200 Loss 0.1839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 Loss train 0.209362 Loss val 1.213857\n",
      "Epoch 61 Batch 0 Loss 0.2291\n",
      "Epoch 61 Batch 100 Loss 0.2034\n",
      "Epoch 61 Batch 200 Loss 0.1909\n",
      "Epoch 61 Loss train 0.206511 Loss val 1.235025\n",
      "Epoch 62 Batch 0 Loss 0.1844\n",
      "Epoch 62 Batch 100 Loss 0.1939\n",
      "Epoch 62 Batch 200 Loss 0.1869\n",
      "Epoch 62 Loss train 0.207685 Loss val 1.230956\n",
      "Epoch 63 Batch 0 Loss 0.2084\n",
      "Epoch 63 Batch 100 Loss 0.1985\n",
      "Epoch 63 Batch 200 Loss 0.2131\n",
      "Epoch 63 Loss train 0.207320 Loss val 1.198919\n",
      "Epoch 64 Batch 0 Loss 0.1955\n",
      "Epoch 64 Batch 100 Loss 0.2033\n",
      "Epoch 64 Batch 200 Loss 0.1872\n",
      "Epoch 64 Loss train 0.201368 Loss val 1.240959\n",
      "Epoch 65 Batch 0 Loss 0.1947\n",
      "Epoch 65 Batch 100 Loss 0.1967\n",
      "Epoch 65 Batch 200 Loss 0.2096\n",
      "Epoch 65 Loss train 0.212680 Loss val 1.228116\n",
      "Epoch 66 Batch 0 Loss 0.1981\n",
      "Epoch 66 Batch 100 Loss 0.2109\n",
      "Epoch 66 Batch 200 Loss 0.2259\n",
      "Epoch 66 Loss train 0.216316 Loss val 1.211239\n",
      "Epoch 67 Batch 0 Loss 0.1893\n",
      "Epoch 67 Batch 100 Loss 0.1796\n",
      "Epoch 67 Batch 200 Loss 0.1963\n",
      "Epoch 67 Loss train 0.201423 Loss val 1.190792\n",
      "Epoch 68 Batch 0 Loss 0.1682\n",
      "Epoch 68 Batch 100 Loss 0.1908\n",
      "Epoch 68 Batch 200 Loss 0.1850\n",
      "Epoch 68 Loss train 0.197113 Loss val 1.182709\n",
      "Epoch 69 Batch 0 Loss 0.1946\n",
      "Epoch 69 Batch 100 Loss 0.1902\n",
      "Epoch 69 Batch 200 Loss 0.1871\n",
      "Epoch 69 Loss train 0.194852 Loss val 1.221419\n",
      "Epoch 70 Batch 0 Loss 0.1693\n",
      "Epoch 70 Batch 100 Loss 0.1826\n",
      "Epoch 70 Batch 200 Loss 0.1803\n",
      "Epoch 70 Loss train 0.194317 Loss val 1.191747\n",
      "Epoch 71 Batch 0 Loss 0.1990\n",
      "Epoch 71 Batch 100 Loss 0.2080\n",
      "Epoch 71 Batch 200 Loss 0.1932\n",
      "Epoch 71 Loss train 0.201904 Loss val 1.208271\n",
      "Epoch 72 Batch 0 Loss 0.1872\n",
      "Epoch 72 Batch 100 Loss 0.1819\n",
      "Epoch 72 Batch 200 Loss 0.1869\n",
      "Epoch 72 Loss train 0.200409 Loss val 1.234223\n",
      "Epoch 73 Batch 0 Loss 0.1921\n",
      "Epoch 73 Batch 100 Loss 0.2085\n",
      "Epoch 73 Batch 200 Loss 0.1830\n",
      "Epoch 73 Loss train 0.202192 Loss val 1.191960\n",
      "Epoch 74 Batch 0 Loss 0.1825\n",
      "Epoch 74 Batch 100 Loss 0.1879\n",
      "Epoch 74 Batch 200 Loss 0.1787\n",
      "Epoch 74 Loss train 0.193066 Loss val 1.196814\n",
      "Epoch 75 Batch 0 Loss 0.1750\n",
      "Epoch 75 Batch 100 Loss 0.1946\n",
      "Epoch 75 Batch 200 Loss 0.1874\n",
      "Epoch 75 Loss train 0.198382 Loss val 1.169882\n",
      "Epoch 76 Batch 0 Loss 0.1765\n",
      "Epoch 76 Batch 100 Loss 0.1913\n",
      "Epoch 76 Batch 200 Loss 0.1832\n",
      "Epoch 76 Loss train 0.188288 Loss val 1.164581\n",
      "Epoch 77 Batch 0 Loss 0.1936\n",
      "Epoch 77 Batch 100 Loss 0.1819\n",
      "Epoch 77 Batch 200 Loss 0.1897\n",
      "Epoch 77 Loss train 0.190655 Loss val 1.183686\n",
      "Epoch 78 Batch 0 Loss 0.1737\n",
      "Epoch 78 Batch 100 Loss 0.1883\n",
      "Epoch 78 Batch 200 Loss 0.1665\n",
      "Epoch 78 Loss train 0.187074 Loss val 1.181136\n",
      "Epoch 79 Batch 0 Loss 0.1793\n",
      "Epoch 79 Batch 100 Loss 0.1761\n",
      "Epoch 79 Batch 200 Loss 0.1847\n",
      "Epoch 79 Loss train 0.188114 Loss val 1.171645\n",
      "Epoch 80 Batch 0 Loss 0.1839\n",
      "Epoch 80 Batch 100 Loss 0.1900\n",
      "Epoch 80 Batch 200 Loss 0.1742\n",
      "Epoch 80 Loss train 0.185116 Loss val 1.171751\n",
      "Epoch 81 Batch 0 Loss 0.1698\n",
      "Epoch 81 Batch 100 Loss 0.1709\n",
      "Epoch 81 Batch 200 Loss 0.1863\n",
      "Epoch 81 Loss train 0.190178 Loss val 1.160545\n",
      "Epoch 82 Batch 0 Loss 0.1963\n",
      "Epoch 82 Batch 100 Loss 0.2098\n",
      "Epoch 82 Batch 200 Loss 0.1789\n",
      "Epoch 82 Loss train 0.187385 Loss val 1.179599\n",
      "Epoch 83 Batch 0 Loss 0.1846\n",
      "Epoch 83 Batch 100 Loss 0.1768\n",
      "Epoch 83 Batch 200 Loss 0.1752\n",
      "Epoch 83 Loss train 0.188927 Loss val 1.190728\n",
      "Epoch 84 Batch 0 Loss 0.1775\n",
      "Epoch 84 Batch 100 Loss 0.1697\n",
      "Epoch 84 Batch 200 Loss 0.1686\n",
      "Epoch 84 Loss train 0.183085 Loss val 1.160016\n",
      "Epoch 85 Batch 0 Loss 0.1742\n",
      "Epoch 85 Batch 100 Loss 0.1779\n",
      "Epoch 85 Batch 200 Loss 0.1860\n",
      "Epoch 85 Loss train 0.179520 Loss val 1.191620\n",
      "Epoch 86 Batch 0 Loss 0.1841\n",
      "Epoch 86 Batch 100 Loss 0.1639\n",
      "Epoch 86 Batch 200 Loss 0.1811\n",
      "Epoch 86 Loss train 0.180119 Loss val 1.149533\n",
      "Epoch 87 Batch 0 Loss 0.1612\n",
      "Epoch 87 Batch 100 Loss 0.1975\n",
      "Epoch 87 Batch 200 Loss 0.2063\n",
      "Epoch 87 Loss train 0.189918 Loss val 1.180890\n",
      "Epoch 88 Batch 0 Loss 0.1845\n",
      "Epoch 88 Batch 100 Loss 0.1748\n",
      "Epoch 88 Batch 200 Loss 0.1926\n",
      "Epoch 88 Loss train 0.187447 Loss val 1.148664\n",
      "Epoch 89 Batch 0 Loss 0.1599\n",
      "Epoch 89 Batch 100 Loss 0.1768\n",
      "Epoch 89 Batch 200 Loss 0.1960\n",
      "Epoch 89 Loss train 0.187202 Loss val 1.144421\n",
      "Epoch 90 Batch 0 Loss 0.1594\n",
      "Epoch 90 Batch 100 Loss 0.1724\n",
      "Epoch 90 Batch 200 Loss 0.1686\n",
      "Epoch 90 Loss train 0.178857 Loss val 1.146452\n",
      "Epoch 91 Batch 0 Loss 0.1620\n",
      "Epoch 91 Batch 100 Loss 0.1785\n",
      "Epoch 91 Batch 200 Loss 0.1749\n",
      "Epoch 91 Loss train 0.184820 Loss val 1.134428\n",
      "Epoch 92 Batch 0 Loss 0.1618\n",
      "Epoch 92 Batch 100 Loss 0.1835\n",
      "Epoch 92 Batch 200 Loss 0.1784\n",
      "Epoch 92 Loss train 0.181284 Loss val 1.091843\n",
      "Epoch 93 Batch 0 Loss 0.1644\n",
      "Epoch 93 Batch 100 Loss 0.1802\n",
      "Epoch 93 Batch 200 Loss 0.1759\n",
      "Epoch 93 Loss train 0.179734 Loss val 1.148360\n",
      "Epoch 94 Batch 0 Loss 0.1666\n",
      "Epoch 94 Batch 100 Loss 0.1636\n",
      "Epoch 94 Batch 200 Loss 0.1786\n",
      "Epoch 94 Loss train 0.175975 Loss val 1.169042\n",
      "Epoch 95 Batch 0 Loss 0.1833\n",
      "Epoch 95 Batch 100 Loss 0.1789\n",
      "Epoch 95 Batch 200 Loss 0.1822\n",
      "Epoch 95 Loss train 0.181909 Loss val 1.164462\n",
      "Epoch 96 Batch 0 Loss 0.1566\n",
      "Epoch 96 Batch 100 Loss 0.1753\n",
      "Epoch 96 Batch 200 Loss 0.1733\n",
      "Epoch 96 Loss train 0.177998 Loss val 1.133306\n",
      "Epoch 97 Batch 0 Loss 0.1635\n",
      "Epoch 97 Batch 100 Loss 0.1740\n",
      "Epoch 97 Batch 200 Loss 0.1752\n",
      "Epoch 97 Loss train 0.174708 Loss val 1.143907\n",
      "Epoch 98 Batch 0 Loss 0.1585\n",
      "Epoch 98 Batch 100 Loss 0.1723\n",
      "Epoch 98 Batch 200 Loss 0.1603\n",
      "Epoch 98 Loss train 0.174881 Loss val 1.126631\n",
      "Epoch 99 Batch 0 Loss 0.1381\n",
      "Epoch 99 Batch 100 Loss 0.1674\n",
      "Epoch 99 Batch 200 Loss 0.1620\n",
      "Epoch 99 Loss train 0.172178 Loss val 1.146370\n",
      "Epoch 100 Batch 0 Loss 0.1652\n",
      "Epoch 100 Batch 100 Loss 0.1833\n",
      "Epoch 100 Batch 200 Loss 0.1628\n",
      "Epoch 100 Loss train 0.178350 Loss val 1.121943\n",
      "CPU times: user 7d 29min 19s, sys: 5d 8h 21min 12s, total: 12d 8h 50min 31s\n",
      "Wall time: 2d 44min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss_plot = []\n",
    "val_loss_plot = []\n",
    "\n",
    "print('Start learning')\n",
    "for epoch in range(0, 100):\n",
    "    total_loss = 0\n",
    "    total_loss_vall = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor.numpy(), target.numpy())\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps_train)\n",
    "    \n",
    "    for (batch_val, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_val_loss, val_loss = validation_step(img_tensor, target)\n",
    "        total_loss_vall += val_loss\n",
    "    val_loss_plot.append(total_loss_vall / num_steps_val)\n",
    "    \n",
    "    # save model after each run\n",
    "    ckpt_manager.save()\n",
    "    \n",
    "    print(f'Epoch {epoch+1} Loss train {total_loss/num_steps_train:.6f} Loss val {total_loss_vall/num_steps_val:.6f}')\n",
    "    #print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19095051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(figsize = (7,7))\n",
    "# make a plot\n",
    "ax.plot(loss_plot, color=\"red\", marker=\"o\")\n",
    "# set x-axis label\n",
    "ax.set_xlabel(\"Epochs\", fontsize = 14)\n",
    "# set y-axis label\n",
    "ax.set_ylabel(\"Train Loss\", color=\"red\", fontsize=14)\n",
    "\n",
    "# twin object for two different y-axis on the sample plot\n",
    "ax2=ax.twinx()\n",
    "# make a plot with different y-axis using second axis object\n",
    "ax2.plot(val_loss_plot, color=\"blue\", marker=\"x\")\n",
    "ax2.set_ylabel(\"Validation Loss\", color=\"blue\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a33903d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "- Progress: 0.0%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mateusz/anaconda3/envs/framework/lib/python3.7/site-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Progress: 100.0%####################\n",
      "\tMetrics train set\n",
      "BLEU-1:  0.56 +- 0.27\n",
      "BLEU-2:  0.44 +- 0.29\n",
      "BLEU-3:  0.35 +- 0.3\n",
      "BLEU-4:  0.28 +- 0.28\n",
      "METEOR:  0.44 +- 0.25\n",
      "ROUGE-1:  0.28 +- 0.21\n",
      "ROUGE-2:  0.11 +- 0.19\n",
      "ROUGE-L:  0.27 +- 0.21\n",
      "- Progress: 0.0%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mateusz/anaconda3/envs/framework/lib/python3.7/site-packages/ipykernel_launcher.py:91: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Progress: 100.0%####################\n",
      "\tMetrics val set\n",
      "BLEU-1:  0.29 +- 0.2\n",
      "BLEU-2:  0.13 +- 0.15\n",
      "BLEU-3:  0.08 +- 0.1\n",
      "BLEU-4:  0.06 +- 0.07\n",
      "METEOR:  0.2 +- 0.16\n",
      "ROUGE-1:  0.15 +- 0.14\n",
      "ROUGE-2:  0.03 +- 0.07\n",
      "ROUGE-L:  0.14 +- 0.14\n"
     ]
    }
   ],
   "source": [
    "bleu_1_train = []; bleu_2_train = []; bleu_3_train = []; bleu_4_train = []\n",
    "meteor_train = []\n",
    "rouge_1_train = []; rouge_2_train = []; rouge_L_train = []\n",
    "cider_train = []\n",
    "scorer_rouge = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "chencherry = SmoothingFunction()\n",
    "\n",
    "image_dataset_train_numpy = np.array(image_dataset_train)\n",
    "captions_dataset_train_numpy = np.array(captions_dataset_train)\n",
    "images = set(image_dataset_train)\n",
    "\n",
    "train_prediction = {}\n",
    "counter = 0\n",
    "for element in images:\n",
    "    utiles.print_progress(counter, len(images))\n",
    "\n",
    "    index = np.where(image_dataset_train_numpy == element)\n",
    "    # data\n",
    "    image_val = np.load(element)\n",
    "    reference_captions = captions_dataset_train_numpy[index]\n",
    "    caption_val_raw = []\n",
    "    caption_val_split = []\n",
    "    for el in reference_captions: \n",
    "        cap = tokenizer.tokens_to_string(el[1:-1])\n",
    "        caption_val_raw.append(cap)\n",
    "        caption_val_split.append(cap.split())\n",
    "    \n",
    "    # make prediction\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "    features = encoder(image_val)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index[MARK_END.strip()]], 0)\n",
    "    prediction_caption = []\n",
    "\n",
    "    for i in range(CAPTION_LENGTH):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input,\n",
    "                                                         features,\n",
    "                                                         hidden)\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        predicted_word = tf.compat.as_text(tokenizer.index_to_word[predicted_id])\n",
    "        prediction_caption.append(predicted_word)\n",
    "\n",
    "        if predicted_word == MARK_END:\n",
    "            prediction_caption = prediction_caption[:-1]\n",
    "            break\n",
    "            \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    # calculate matrics\n",
    "    # Calculate BLEU-1 score\n",
    "    bleu_1_train.append(sentence_bleu(caption_val_split, prediction_caption, weights=(1, 0, 0, 0), smoothing_function=chencherry.method1))\n",
    "    # Calculate BLEU-2 score\n",
    "    bleu_2_train.append(sentence_bleu(caption_val_split, prediction_caption, weights=(0.5, 0.5, 0, 0), smoothing_function=chencherry.method1))\n",
    "    # Calculate BLEU-3 score\n",
    "    bleu_3_train.append(sentence_bleu(caption_val_split, prediction_caption, weights=(1/3, 1/3, 1/3, 0), smoothing_function=chencherry.method1))\n",
    "    # Calculate BLEU-4 score\n",
    "    bleu_4_train.append(sentence_bleu(caption_val_split, prediction_caption, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=chencherry.method1))\n",
    "    # calculate Meteor\n",
    "    meteor_train.append(meteor_score(caption_val_split, prediction_caption))\n",
    "    # rouge score\n",
    "    for el in caption_val_raw:\n",
    "        rouge_scores = scorer_rouge.score(el, ' '.join(prediction_caption))\n",
    "        rouge_1_train.append(rouge_scores['rouge1'].fmeasure)\n",
    "        rouge_2_train.append(rouge_scores['rouge2'].fmeasure)\n",
    "        rouge_L_train.append(rouge_scores['rougeL'].fmeasure)\n",
    "\n",
    "    counter += 1\n",
    "    key_img = element.split('/')[-1][:-4]\n",
    "    train_prediction[key_img] =  ' '.join(prediction_caption)\n",
    "\n",
    "print(\"#\"*20)\n",
    "print(\"\\tMetrics train set\")\n",
    "print(\"BLEU-1: \", np.round(np.mean(bleu_1_train),2), '+-', np.round(np.std(bleu_1_train),2))\n",
    "print(\"BLEU-2: \", np.round(np.mean(bleu_2_train),2), '+-', np.round(np.std(bleu_2_train),2))\n",
    "print(\"BLEU-3: \", np.round(np.mean(bleu_3_train),2), '+-', np.round(np.std(bleu_3_train),2))\n",
    "print(\"BLEU-4: \", np.round(np.mean(bleu_4_train),2), '+-', np.round(np.std(bleu_4_train),2))\n",
    "print(\"METEOR: \", np.round(np.mean(meteor_train),2), '+-', np.round(np.std(meteor_train),2))\n",
    "print(\"ROUGE-1: \", np.round(np.mean(rouge_1_train),2), '+-', np.round(np.std(rouge_1_train),2))\n",
    "print(\"ROUGE-2: \", np.round(np.mean(rouge_2_train),2), '+-', np.round(np.std(rouge_2_train),2))\n",
    "print(\"ROUGE-L: \", np.round(np.mean(rouge_L_train),2), '+-', np.round(np.std(rouge_L_train),2))\n",
    "\n",
    "# metrics calculation for validation set\n",
    "cider_val = []\n",
    "bleu_1_val = []; bleu_2_val = []; bleu_3_val = []; bleu_4_val = []\n",
    "meteor_val = []\n",
    "rouge_1_val = []; rouge_2_val = []; rouge_L_val = []\n",
    "scorer_rouge = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "chencherry = SmoothingFunction()\n",
    "\n",
    "image_dataset_val_numpy = np.array(image_dataset_val)\n",
    "captions_dataset_val_numpy = np.array(captions_dataset_val)\n",
    "images = set(image_dataset_val)\n",
    "\n",
    "val_prediction = {}\n",
    "counter = 0\n",
    "for element in images:\n",
    "    utiles.print_progress(counter, len(images))\n",
    "\n",
    "    index = np.where(image_dataset_val_numpy == element)\n",
    "    # data\n",
    "    image_val = np.load(element)\n",
    "    reference_captions = captions_dataset_val_numpy[index]\n",
    "    caption_val_raw = []\n",
    "    caption_val_split = []\n",
    "    for el in reference_captions: \n",
    "        cap = tokenizer.tokens_to_string(el[1:-1])\n",
    "        caption_val_raw.append(cap)\n",
    "        caption_val_split.append(cap.split())\n",
    "    \n",
    "    # make prediction\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "    features = encoder(image_val)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index[MARK_END.strip()]], 0)\n",
    "    prediction_caption = []\n",
    "\n",
    "    for i in range(CAPTION_LENGTH):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input,\n",
    "                                                         features,\n",
    "                                                         hidden)\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        predicted_word = tf.compat.as_text(tokenizer.index_to_word[predicted_id])\n",
    "        prediction_caption.append(predicted_word)\n",
    "\n",
    "        if predicted_word == MARK_END:\n",
    "            prediction_caption = prediction_caption[:-1]\n",
    "            break\n",
    "            \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    # calculate matrics\n",
    "    # Calculate BLEU-1 score\n",
    "    bleu_1_val.append(sentence_bleu(caption_val_split, prediction_caption, weights=(1, 0, 0, 0), smoothing_function=chencherry.method1))\n",
    "    # Calculate BLEU-2 score\n",
    "    bleu_2_val.append(sentence_bleu(caption_val_split, prediction_caption, weights=(0.5, 0.5, 0, 0), smoothing_function=chencherry.method1))\n",
    "    # Calculate BLEU-3 score\n",
    "    bleu_3_val.append(sentence_bleu(caption_val_split, prediction_caption, weights=(1/3, 1/3, 1/3, 0), smoothing_function=chencherry.method1))\n",
    "    # Calculate BLEU-4 score\n",
    "    bleu_4_val.append(sentence_bleu(caption_val_split, prediction_caption, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=chencherry.method1))\n",
    "    # calculate Meteor\n",
    "    meteor_val.append(meteor_score(caption_val_split, prediction_caption))\n",
    "    # rouge score\n",
    "    for el in caption_val_raw:\n",
    "        rouge_scores = scorer_rouge.score(el, ' '.join(prediction_caption))\n",
    "        rouge_1_val.append(rouge_scores['rouge1'].fmeasure)\n",
    "        rouge_2_val.append(rouge_scores['rouge2'].fmeasure)\n",
    "        rouge_L_val.append(rouge_scores['rougeL'].fmeasure)\n",
    "    \n",
    "    key_img = element.split('/')[-1][:-4]\n",
    "    val_prediction[key_img] =  ' '.join(prediction_caption)\n",
    "    counter +=1\n",
    " \n",
    "print(\"#\"*20)\n",
    "print(\"\\tMetrics val set\")\n",
    "print(\"BLEU-1: \", np.round(np.mean(bleu_1_val),2), '+-', np.round(np.std(bleu_1_val),2))\n",
    "print(\"BLEU-2: \", np.round(np.mean(bleu_2_val),2), '+-', np.round(np.std(bleu_2_val),2))\n",
    "print(\"BLEU-3: \", np.round(np.mean(bleu_3_val),2), '+-', np.round(np.std(bleu_3_val),2))\n",
    "print(\"BLEU-4: \", np.round(np.mean(bleu_4_val),2), '+-', np.round(np.std(bleu_4_val),2))\n",
    "print(\"METEOR: \", np.round(np.mean(meteor_val),2), '+-', np.round(np.std(meteor_val),2))\n",
    "print(\"ROUGE-1: \", np.round(np.mean(rouge_1_val),2), '+-', np.round(np.std(rouge_1_val),2))\n",
    "print(\"ROUGE-2: \", np.round(np.mean(rouge_2_val),2), '+-', np.round(np.std(rouge_2_val),2))\n",
    "print(\"ROUGE-L: \", np.round(np.mean(rouge_L_val),2), '+-', np.round(np.std(rouge_L_val),2))\n",
    "                                          \n",
    "path_to_save = '/home2/Kacper_captioning/f30_xception_file_va/'\n",
    "\n",
    "with open(path_to_save + 'train_predictions_units_512_v2.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_prediction, fp)\n",
    "with open(path_to_save + 'test_predictions_units_512_v2.pkl', 'wb') as fp:\n",
    "    pickle.dump(val_prediction, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93824a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
